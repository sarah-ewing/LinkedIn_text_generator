{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install requests\n",
    "# !pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "25\n",
      "2\n",
      "50\n",
      "3\n",
      "75\n",
      "4\n",
      "100\n",
      "5\n",
      "125\n",
      "6\n",
      "150\n",
      "7\n",
      "175\n",
      "8\n",
      "200\n",
      "9\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "final = pd.DataFrame()\n",
    "\n",
    "for ii in range(1, 10):\n",
    "    print(ii)\n",
    "    i = str(ii * 25)\n",
    "    print(i)\n",
    "    URL = 'https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=United%20States&start='\n",
    "    page = requests.get(URL+i)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    job_elems = soup.find_all(class_='result-card__full-card-link')\n",
    "\n",
    "    for job_elem in job_elems:\n",
    "\n",
    "        new_url = job_elem.find('span', class_='screen-reader-text')\n",
    "        result21 = str(job_elem).find('</span></a></a>')\n",
    "    #     print(\"\\n job_title:\",str(new_url)[33:(result21-6)])\n",
    "\n",
    "        result12 = str(job_elem).find('href=')\n",
    "        result22 = str(job_elem).find('\"><span class=\"')\n",
    "    #     print(\"\\n individual_url:\", str(job_elem)[(result12+6):result22])\n",
    "\n",
    "        Single_url = str(job_elem)[(result12+6):result22]\n",
    "        page11 = requests.get(Single_url)\n",
    "        soup_look = BeautifulSoup(page11.content, 'html.parser').prettify()\n",
    "\n",
    "        result13 = str(soup_look).find('{\"@context\":')\n",
    "        result23 = str(soup_look).find('\",\"validThrough\":\"')\n",
    "        long_desc = str(soup_look)[(result13+12):(result23)]\n",
    "        long_desc = long_desc.replace(u'\\\\u003C/li\\\\u003E\\\\u003Cli\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003Cbr\\\\u003E\\\\u003C/li\\\\u003E\\\\u003C/ul\\\\u003E\\\\u003Cem\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003Cbr\\\\u003E\\\\u003C/u\\\\u003E\\\\u003C/strong', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003Cbr\\\\u003E\\\\u003Cbr\\\\u003E\\\\u003Cstrong\\\\u003E\\\\u003Cu\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003Cbr\\\\u003E\\\\u003Cbr\\\\u003E\\\\u003C/li\\\\u003E\\\\u003C/ul\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003Cli\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003C/li\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace('\\\\u003C/li\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003Cbr\\\\u003E\\\\u003Cbr\\\\u003E', \" \")\n",
    "        long_desc = long_desc.replace(u'\\\\u003C', \"\")\n",
    "        long_desc = long_desc.replace(u'\\\\u003E', \"\")\n",
    "        long_desc = long_desc.replace(u'/li', \"\")\n",
    "        long_desc = long_desc.replace(u'gobr/ul10%strong', \"\")\n",
    "        long_desc = long_desc.replace(u'/strongstrong', \"\")\n",
    "        long_desc = long_desc.replace(u'/ul/ulstrongu', \"\")\n",
    "        long_desc = long_desc.replace(u'br/strongulli', \"\")\n",
    "        long_desc = long_desc.replace(u'/u/strongu', \"\")\n",
    "        long_desc = long_desc.replace(u'/ulstrongu', \"\")\n",
    "        long_desc = long_desc.replace(u'br/strongli', \"\")\n",
    "        long_desc = long_desc.replace(u'br/ulstrong', \"\")\n",
    "        long_desc = long_desc.replace(u'/strongul', \"\")\n",
    "        long_desc = long_desc.replace(u'listrong', \"\")\n",
    "        long_desc = long_desc.replace(u'/strong', \"\")\n",
    "        long_desc = long_desc.replace(u'strongu', \"\")\n",
    "        long_desc = long_desc.replace(u'brstrong', \"\")\n",
    "        long_desc = long_desc.replace(u' li ', \"\")\n",
    "        long_desc = long_desc.replace(u'  ', \" \")\n",
    "\n",
    "        result14 = str(soup_look).find(',\"validThrough\":\"')\n",
    "    #     print(\"\\n end_date:\", str(soup_look)[(result14+17):(result14+41)])\n",
    "\n",
    "        result15 = str(long_desc).find('\",\"datePosted\":\"')\n",
    "        result25 = str(long_desc).find('\",\"description\":')\n",
    "    #     print(\"\\n date_posted:\", long_desc[(result15+16):result25])\n",
    "\n",
    "        result26 = str(long_desc).find('\",\"employmentType\"')\n",
    "    #     print(\"\\n description:\", long_desc[(result25+17):result26])\n",
    "\n",
    "        result17 = str(long_desc).find('\",\"employmentType\"')\n",
    "        result27 = str(long_desc).find('\",\"experienceRequirements\"')\n",
    "    #     print(\"\\n employmentType:\", long_desc[(result17+20):result27])\n",
    "\n",
    "        result18 = str(long_desc).find('\",\"experienceRequirements\":\"')\n",
    "        result28 = str(long_desc).find('\",\"hiringOrganization\"')\n",
    "#         print(\"\\n experienceRequirements:\", long_desc[(result18+28):result28])\n",
    "\n",
    "        result19 = str(long_desc).find('\"hiringOrganization\":{\"@type\":\"Organization\",\"name\":\"')\n",
    "        result29 = str(long_desc).find('\",\"sameAs\":')\n",
    "    #     print(\"\\n hiringOrganization:\", long_desc[(result19+53):result29])\n",
    "\n",
    "        result110 = str(long_desc).find(',\"industry\":\"')\n",
    "        result210 = str(long_desc).find('\",\"jobLocation\":{\"')\n",
    "    #     print(\"\\n hiringOrganization:\", long_desc[(result110+13):result210])\n",
    "\n",
    "        result111 = str(long_desc).find('\"addressLocality\":\"')\n",
    "        result211 = str(long_desc).find('\",\"addressRegion\":')\n",
    "    #     print(\"\\n addressLocality:\", long_desc[(result111+19):result211])\n",
    "\n",
    "        result112 = str(long_desc).find(',\"addressRegion\":')\n",
    "        result212 = str(long_desc).find(',\"postalCode\":')\n",
    "        addressRegion = long_desc[(result112+17):result212]\n",
    "        addressRegion = addressRegion.replace('\"', \"\")\n",
    "        if len(addressRegion) >=3:\n",
    "            addressRegion = None\n",
    "    #     print(\"\\n addressRegion:\", long_desc[(result112+17):result212])\n",
    "\n",
    "        result113 = str(long_desc).find('\"postalCode\":\"')\n",
    "        result213 = str(long_desc).find('\",\"addressCountry\":\"')\n",
    "    #     print(\"\\n postalCode:\", long_desc[(result113+14):result213])\n",
    "\n",
    "        result114 = str(long_desc).find('\",\"addressCountry\":\"')\n",
    "        result214 = str(long_desc).find('\"}},\"')\n",
    "    #     print(\"\\n addressCountry:\", long_desc[(result114+20):result214])\n",
    "\n",
    "    ### put it all into a data frame\n",
    "        column = ['job_title', 'individual_url', 'end_date', 'date_posted', \n",
    "                   'description', 'employmentType', 'experienceRequirements',\n",
    "                  'hiringOrganization','hiringOrganization', 'addressLocality',\n",
    "                  'addressRegion', 'postalCode','addressCountry']\n",
    "\n",
    "        data = [[str(new_url)[33:(result21-6)], \n",
    "                str(job_elem)[(result12+6):result22],\n",
    "                str(soup_look)[(result14+17):(result14+41)],\n",
    "                long_desc[(result15+16):result25],\n",
    "                long_desc[(result25+17):result26],\n",
    "                long_desc[(result17+20):result27],\n",
    "                long_desc[(result18+28):result28],\n",
    "                long_desc[(result19+53):result29],\n",
    "                long_desc[(result110+13):result210],\n",
    "                long_desc[(result111+19):result211],\n",
    "                addressRegion,\n",
    "                long_desc[(result113+14):result213],\n",
    "                long_desc[(result114+20):result214]]]\n",
    "        df = pd.DataFrame(data, columns = column)\n",
    "\n",
    "#         print(df)\n",
    "        final = final.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>individual_url</th>\n",
       "      <th>end_date</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>description</th>\n",
       "      <th>employmentType</th>\n",
       "      <th>experienceRequirements</th>\n",
       "      <th>hiringOrganization</th>\n",
       "      <th>hiringOrganization</th>\n",
       "      <th>addressLocality</th>\n",
       "      <th>addressRegion</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>addressCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence/Machine Learning Engineer Associate with Security Clearance</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/artificial-intelligence-machine-learning-engineer-associate-with-security-clearance-at-clearancejobs-2268834870?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;amp;trackingId=DfvmWF62meFKCj6KZ4O06w%3D%3D&amp;amp;position=1&amp;amp;pageNum=0&amp;amp;trk=public_jobs_job-result-card_result-card_full-click</td>\n",
       "      <td>2021-01-04T20:30:54.000Z</td>\n",
       "      <td>2020-11-10T00:00:00.000Z</td>\n",
       "      <td>Why choose between doing meaningful work and having a fulfilling life? At MITRE, you can have both. That's because MITRE people are committed to tackling our nation's toughest challenges-and we're committed to the long-term well-being of our employees. MITRE is different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&amp;amp;D centers we operate for the government create la...</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>ClearanceJobs</td>\n",
       "      <td>Restaurants,Financial Services,Hospitality</td>\n",
       "      <td>Mclean</td>\n",
       "      <td>VA</td>\n",
       "      <td>22101</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Engineer - Cortex Applied Research - User Modeling</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/machine-learning-engineer-cortex-applied-research-user-modeling-at-twitter-2314599088?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;amp;trackingId=Y%2F3L6jl%2BpLQrgoF31FUJUw%3D%3D&amp;amp;position=2&amp;amp;pageNum=0&amp;amp;trk=public_jobs_job-result-card_result-card_full-click</td>\n",
       "      <td>2021-01-02T14:57:14.000Z</td>\n",
       "      <td>2020-12-03T14:57:14.000Z</td>\n",
       "      <td>Company Descriptionbr Twitter serves the public conversation by encouraging people all over the world to connect, learn, debate, and solve problems together. We believe conversation can change the world, and that’s why Tweeps (that’s what we call Twitter employees) come to work every day. Job Descriptionbr The Cortex Applied Research group was created to support product teams across the company with effectively leveraging Machine Learning to solve customer problems. Importantly, the Applied ...</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Internet</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>94101</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Data Machine Learning Engineer</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/big-data-machine-learning-engineer-at-mediant-health-resources-2181311096?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;amp;trackingId=2f7gwm1DuON7%2Fv%2BSSyeOUQ%3D%3D&amp;amp;position=3&amp;amp;pageNum=0&amp;amp;trk=public_jobs_job-result-card_result-card_full-click</td>\n",
       "      <td>2020-12-28T17:20:50.000Z</td>\n",
       "      <td>2020-11-13T10:33:08.000Z</td>\n",
       "      <td>Job Descriptionbr strongLocation: Cleveland, OH strongDuration: Permanent Responsibilities ul Bachelor’s degree in computer science, electrical/electronic engineering or other engineering or technical discipline is required.  Minimum of 8 years of experience in IT and Big data software development is required  Minimum 3+ Predictive Analytics model implementation experience in production environments using ML/DL libraries like TensorFlow, H20, Pytorch, Sci-kit Learn.  Experience in using NLP,...</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Mediant Health Resources</td>\n",
       "      <td>Hospital &amp; Health Care,Information Technology and Services</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>OH</td>\n",
       "      <td>44101</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence and Machine Learning Engineer</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/artificial-intelligence-and-machine-learning-engineer-at-colsa-2321349014?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;amp;trackingId=OvlMvZq0TKiJ5aeCKxkhGA%3D%3D&amp;amp;position=4&amp;amp;pageNum=0&amp;amp;trk=public_jobs_job-result-card_result-card_full-click</td>\n",
       "      <td>2021-01-05T23:36:55.000Z</td>\n",
       "      <td>2020-10-31T03:27:14.000Z</td>\n",
       "      <td>General Summarybr Conducts highly specialized, vital activities regarding the design, implementation and maintenance of the organization’s application and/or IT infrastructure. strong Principal Duties and Responsibilities (*Essential functions)br Conceptualizes, models, and guides the logical design and development of systemsbr /ularchitectures, and defines key systems capabilities and performance requirements.  Defines total systems design, technology, and interface operational concepts.  D...</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>COLSA</td>\n",
       "      <td>Information Technology and Services,Defense &amp; Space,Computer Software</td>\n",
       "      <td>Huntsville</td>\n",
       "      <td>AL</td>\n",
       "      <td>35801</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist/Machine Learning Engineer</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scientist-machine-learning-engineer-at-diverse-lynx-2268144058?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;amp;trackingId=FREwhZxv8JzQxYQLDVhVfg%3D%3D&amp;amp;position=5&amp;amp;pageNum=0&amp;amp;trk=public_jobs_job-result-card_result-card_full-click</td>\n",
       "      <td>2021-01-01T16:25:06.000Z</td>\n",
       "      <td>2020-10-08T15:46:16.000Z</td>\n",
       "      <td>strongRole: Data Scientist/Machine Learning EngineerbrLocation: Boston, MAbrContract position/Full time position uJob Descriptionbr Python Development experiencebrul strongemData processing/em is Mandatory.brDocker, CI/CD experience  Develop and consume REST APIs using Python Frameworks Django/Flask.  Writing unit test cases using python test modules like pytest, unittest.  Experience in any relational or NoSQL databases, good to have experience in Oracle/MongoDB.  Strong technical, analytic...</td>\n",
       "      <td>CONTRACTOR</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Diverse Lynx</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>02101</td>\n",
       "      <td>None</td>\n",
       "      <td>02101</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             job_title  \\\n",
       "0  Artificial Intelligence/Machine Learning Engineer Associate with Security Clearance   \n",
       "0                  Machine Learning Engineer - Cortex Applied Research - User Modeling   \n",
       "0                                                   Big Data Machine Learning Engineer   \n",
       "0                                Artificial Intelligence and Machine Learning Engineer   \n",
       "0                                             Data Scientist/Machine Learning Engineer   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                      individual_url  \\\n",
       "0  https://www.linkedin.com/jobs/view/artificial-intelligence-machine-learning-engineer-associate-with-security-clearance-at-clearancejobs-2268834870?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;trackingId=DfvmWF62meFKCj6KZ4O06w%3D%3D&amp;position=1&amp;pageNum=0&amp;trk=public_jobs_job-result-card_result-card_full-click   \n",
       "0                        https://www.linkedin.com/jobs/view/machine-learning-engineer-cortex-applied-research-user-modeling-at-twitter-2314599088?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;trackingId=Y%2F3L6jl%2BpLQrgoF31FUJUw%3D%3D&amp;position=2&amp;pageNum=0&amp;trk=public_jobs_job-result-card_result-card_full-click   \n",
       "0                                    https://www.linkedin.com/jobs/view/big-data-machine-learning-engineer-at-mediant-health-resources-2181311096?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;trackingId=2f7gwm1DuON7%2Fv%2BSSyeOUQ%3D%3D&amp;position=3&amp;pageNum=0&amp;trk=public_jobs_job-result-card_result-card_full-click   \n",
       "0                                        https://www.linkedin.com/jobs/view/artificial-intelligence-and-machine-learning-engineer-at-colsa-2321349014?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;trackingId=OvlMvZq0TKiJ5aeCKxkhGA%3D%3D&amp;position=4&amp;pageNum=0&amp;trk=public_jobs_job-result-card_result-card_full-click   \n",
       "0                                              https://www.linkedin.com/jobs/view/data-scientist-machine-learning-engineer-at-diverse-lynx-2268144058?refId=9659da90-9284-4738-833b-9f2be45251fc&amp;trackingId=FREwhZxv8JzQxYQLDVhVfg%3D%3D&amp;position=5&amp;pageNum=0&amp;trk=public_jobs_job-result-card_result-card_full-click   \n",
       "\n",
       "                   end_date               date_posted  \\\n",
       "0  2021-01-04T20:30:54.000Z  2020-11-10T00:00:00.000Z   \n",
       "0  2021-01-02T14:57:14.000Z  2020-12-03T14:57:14.000Z   \n",
       "0  2020-12-28T17:20:50.000Z  2020-11-13T10:33:08.000Z   \n",
       "0  2021-01-05T23:36:55.000Z  2020-10-31T03:27:14.000Z   \n",
       "0  2021-01-01T16:25:06.000Z  2020-10-08T15:46:16.000Z   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           description  \\\n",
       "0  Why choose between doing meaningful work and having a fulfilling life? At MITRE, you can have both. That's because MITRE people are committed to tackling our nation's toughest challenges-and we're committed to the long-term well-being of our employees. MITRE is different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&amp;D centers we operate for the government create la...   \n",
       "0  Company Descriptionbr Twitter serves the public conversation by encouraging people all over the world to connect, learn, debate, and solve problems together. We believe conversation can change the world, and that’s why Tweeps (that’s what we call Twitter employees) come to work every day. Job Descriptionbr The Cortex Applied Research group was created to support product teams across the company with effectively leveraging Machine Learning to solve customer problems. Importantly, the Applied ...   \n",
       "0  Job Descriptionbr strongLocation: Cleveland, OH strongDuration: Permanent Responsibilities ul Bachelor’s degree in computer science, electrical/electronic engineering or other engineering or technical discipline is required.  Minimum of 8 years of experience in IT and Big data software development is required  Minimum 3+ Predictive Analytics model implementation experience in production environments using ML/DL libraries like TensorFlow, H20, Pytorch, Sci-kit Learn.  Experience in using NLP,...   \n",
       "0  General Summarybr Conducts highly specialized, vital activities regarding the design, implementation and maintenance of the organization’s application and/or IT infrastructure. strong Principal Duties and Responsibilities (*Essential functions)br Conceptualizes, models, and guides the logical design and development of systemsbr /ularchitectures, and defines key systems capabilities and performance requirements.  Defines total systems design, technology, and interface operational concepts.  D...   \n",
       "0  strongRole: Data Scientist/Machine Learning EngineerbrLocation: Boston, MAbrContract position/Full time position uJob Descriptionbr Python Development experiencebrul strongemData processing/em is Mandatory.brDocker, CI/CD experience  Develop and consume REST APIs using Python Frameworks Django/Flask.  Writing unit test cases using python test modules like pytest, unittest.  Experience in any relational or NoSQL databases, good to have experience in Oracle/MongoDB.  Strong technical, analytic...   \n",
       "\n",
       "  employmentType experienceRequirements        hiringOrganization  \\\n",
       "0      FULL_TIME            Entry level             ClearanceJobs   \n",
       "0      FULL_TIME         Not Applicable                   Twitter   \n",
       "0      FULL_TIME         Not Applicable  Mediant Health Resources   \n",
       "0      FULL_TIME            Entry level                     COLSA   \n",
       "0     CONTRACTOR            Entry level              Diverse Lynx   \n",
       "\n",
       "                                                      hiringOrganization  \\\n",
       "0                             Restaurants,Financial Services,Hospitality   \n",
       "0                                                               Internet   \n",
       "0             Hospital & Health Care,Information Technology and Services   \n",
       "0  Information Technology and Services,Defense & Space,Computer Software   \n",
       "0                                    Information Technology and Services   \n",
       "\n",
       "  addressLocality addressRegion postalCode addressCountry  \n",
       "0          Mclean            VA      22101             US  \n",
       "0   San Francisco            CA      94101             US  \n",
       "0       Cleveland            OH      44101             US  \n",
       "0      Huntsville            AL      35801             US  \n",
       "0           02101          None      02101             US  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Why choose between doing meaningful work and having a fulfilling life? At MITRE, you can have both. That's because MITRE people are committed to tackling our nation's toughest challenges-and we're committed to the long-term well-being of our employees. MITRE is different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&amp;D centers we operate for the government create la...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['description'][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = final['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.layers import LSTM\n",
    "\n",
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(author.values)\n",
    "sequences = tokenizer.texts_to_sequences(author.values)\n",
    "\n",
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5551"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1207 08:55:23.918036 4555210176 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1207 08:55:23.920886 4555210176 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1207 08:55:26.640745 4555210176 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "203136/203223 [============================>.] - ETA: 0s - loss: 6.1927 - acc: 0.0662\n",
      "Epoch 00001: loss improved from inf to 6.19242, saving model to ./model_2_weights.hdf5\n",
      "203223/203223 [==============================] - 137s 676us/sample - loss: 6.1924 - acc: 0.0662\n",
      "Epoch 2/150\n",
      "164224/203223 [=======================>......] - ETA: 23s - loss: 5.2269 - acc: 0.1556"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d3b5f5eaaf81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m          \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m          \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m          verbose = 1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_2 = keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    layers.LSTM(100, return_sequences=True),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Train model with checkpoints\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "filepath = \"./model_2_weights.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model_2.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 150,\n",
    "         batch_size = 128,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model,seq,max_len = 20):\n",
    "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
    "    reaches max_len'''\n",
    "    # Tokenize the input string\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
    "    max_len = max_len+len(tokenized_sent[0])\n",
    "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
    "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
    "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
    "    while len(tokenized_sent[0]) < max_len:\n",
    "        padded_sentence = tf.keras.preprocessing.sequence.pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent[0].append(op.argmax()+1)\n",
    "        \n",
    "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_done = keras.models.load_model('./model_2_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(model = model_done,\n",
    "    seq = \"Tinder brings people together. With tens of millions of users and a presence in every country on earth\",\n",
    "    max_len = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
